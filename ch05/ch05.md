`docker run -d -P nginx`

The `-P` stands for `--publish all` and publishes all the exposed ports by that image to random ports. Docker picks them.
You can see which ports it picked with `docker  ps -l`.

You can also access it from the shell with `curl localhost:<PORT>`, for example.

You can also use `-p` to set the port used by your image: `docker run -d -p <HOST_PORT_NR>:<CONTAINER_PORT_NR> <DOCKER_IMAGE>`

---

There are a couple of network drivers that you can use on your containers:
You select this by using `docker run --net ..`

**bridge(default)**

The container gets a virtual `eth0` interface (virtual network card). It also has a `lo` loopback interface.

Docker is creating a pair of `veth` interfaces; one on the host and one on the container. 
You can actually check this by `sudo apt-get install bridge-utils` if you are running on a unix machine and then using `brctl show`. You'll see the linux bridge `docker0`.

**none**

You only have the `lo` interface now. Can't really connect to the network from that container environment. 

**host**

It essentially accesses the network interfaces of the host. All of them. It can bind any port/any address.


**container**

This re-uses the network stack of another container.
You connect to it by running `docker run --net container:<CONTAINER_ID>`

The containers communicate over the `lo` interface.

---

We also have the `network` tag. By default, we have the host, the bridge(docker) and none (You can check this with `docker network ls`).
A network, conceptually, is like a virtual switch. It also has an IP subnet associated to it.

---

To create a network: `docker network create <NAME>`.
You can    connect containers with `--net <NAME>`. 

The containers will be in the same subnet space. Can check this if you `ping` the other container that resides in the same network, for example.

---

CNI - Container Network Interface

With CNI, al the nodes and containers are on a single IP network.

Essentially, both CNI and CNM (Container Network Model) have the same functionality just with different methods of implementing it.

---

If you need a container that will have the same name across 2 different network scopes (say you have the `dev` network and the `staging` network), you can't really use the same name (say if you need a redis container with the name `redis` in both of the networks).

Instead of using `--name` tag you can use `--network-alias`. It uses the Docker DNS server. This works in parallel.

---

Ambassador pattern:
You basically have containers that "proxy" for another service.

It can help with 
- discovery (where does this run?)
- migration (what if the service moves?)
- load balancing (spreading requests across multiple instances of a service)
- authentication (helps with the separation of concerns)

Example: *connecting a web host to a db* - those two being in two different containers.
They communicate through the help of 2 ambassadors which are totally transparent (no diff. between normal operation and operation with an ambassador). 
Another helpful thing with the ambassador is that if the db moves- the ambassador tracks it so the web host can still interact with it. It's just a nice way of 
providing decoupling.

---

Service meshes

Are a configurable network layer. Particularly useful for microservices. They are often implemented as proxies. 
Applications, therefore, connect to the service mesh which relays the connection where needed.

When using a service mesh, you have a "sidecar container"(essentially an ambassador) which is often used as a proxy.

If we look at it from a different scope, service meshes are essentially app-wide or cluster-wide ambassadors.
